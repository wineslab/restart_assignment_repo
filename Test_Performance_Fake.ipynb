{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["mWq5iGx0JAQF","og2qTObkJGZb","uVtOcrqyHLqh","JQQ_eaT3IfXO"],"authorship_tag":"ABX9TyMeEwT0JOjjJnN6R3VJTfCy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Use this notebook to evaluate performance of supervised and/or unsupervised approach on Test Set."],"metadata":{"id":"lVT1Ypo4GX4N"}},{"cell_type":"markdown","source":["#Import Libraries and Functions"],"metadata":{"id":"mWq5iGx0JAQF"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib as mpl\n","import numpy as np\n","from collections import defaultdict\n","\n","# Supervised Learning\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Clustering\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, silhouette_samples, completeness_score, homogeneity_score\n","\n","\n","# PREPROCESSING\n","\n","def load_dataset(filename):\n","    \"\"\" Load the dataset and handle errors \"\"\"\n","    try:\n","        dataset = pd.read_pickle(filename)\n","        print(f\"Dataset Loaded with {len(dataset)} rows!\")\n","        return dataset\n","    except Exception as e:\n","        print(f\"Could not load the dataset, exiting! Error: {e}\")\n","        exit(-1)\n","\n","\n","def split_and_save_dataset(dataset, extract_rate, filenames):\n","    \"\"\" Split dataset and save the samples \"\"\"\n","    # Sampling a fraction of the dataset\n","    sampled_df = dataset.sample(frac=extract_rate, random_state=42)\n","    remaining_df = dataset.drop(sampled_df.index)\n","\n","    # Resetting indices\n","    sampled_df = sampled_df.reset_index(drop=True)\n","    remaining_df = remaining_df.reset_index(drop=True)\n","\n","    # Save to pickle\n","    sampled_df.to_pickle(filenames['testing'])\n","    remaining_df.to_pickle(filenames['training'])\n","\n","    print(f\"Training dataset saved with {len(remaining_df)} rows!\")\n","    print(f\"Testing dataset saved with {len(sampled_df)} rows!\")\n","\n","\n","def plot_kpi(imsi_data, kpi_column, selected_imsi):\n","    \"\"\" Plot the KPI over time using a stem plot \"\"\"\n","    # Adjust 'Timestamp' to start from 0 using the smallest value as reference\n","    imsi_data['Adjusted_Timestamp'] = (imsi_data['Timestamp'] - imsi_data['Timestamp'].min()).dt.total_seconds()\n","\n","    # Plot\n","    plt.figure(figsize=(10, 6))\n","    plt.stem(imsi_data['Adjusted_Timestamp'], imsi_data[kpi_column], use_line_collection=True)\n","    plt.xlabel('Time [s]')  # Time in seconds\n","    plt.ylabel(kpi_column)\n","    plt.title(f'{kpi_column} Over Time for IMSI: {selected_imsi}')\n","    plt.grid(True)\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","\n","def plot_correlation_matrix(imsi_data):\n","    \"\"\" Plot the correlation matrix excluding certain columns \"\"\"\n","    # Drop unwanted columns\n","    data_to_correlate = imsi_data.drop(columns=[\"Timestamp\", \"IMSI\", \"slice_id\"], errors='ignore')\n","\n","    # Compute the correlation matrix\n","    corr_matrix = np.round(data_to_correlate.corr(),2)\n","\n","    # Plot the correlation matrix\n","    plt.figure(figsize=(12, 8))\n","    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","    plt.title(\"Correlation Matrix for Selected IMSI\")\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","# TRAINING AND PREDICTION\n","\n","\n","def normalize_dataset(X_train, X_test):\n","    mean_x, std_x = X_train.mean(), X_train.std()\n","\n","    X_train_norm = (X_train - mean_x) / std_x\n","    X_test_norm = (X_test - mean_x) / std_x\n","\n","    stats_x = [mean_x, std_x]\n","\n","    return X_train_norm, X_test_norm, stats_x\n","\n","# supervised learning\n","\n","def grid_search(classifier, parameters, train, ground_truth, pred_input, cross_val=3):\n","\n","    gscv = GridSearchCV(classifier, parameters, cv=cross_val,\n","                        n_jobs=12, return_train_score=False,\n","                        verbose=5, scoring='accuracy')\n","    gscv.fit(train, ground_truth)\n","\n","    y_pred = gscv.predict(pred_input)\n","\n","    return gscv, y_pred\n","\n","\n","def get_bestpar_list(bestpar_df):\n","\n","\n","    dd = defaultdict(list)\n","    for index, row in bestpar_df.iterrows():  # list input dicts\n","\n","        bp = row['bestpars']\n","        for key, value in zip(list(bp.keys()), list(bp.values())):\n","            dd[key] = value\n","\n","    return dd\n","\n","def plot_metric_supervised(perf, metric, labels, colors=None):\n","\n","    fig, ax = plt.subplots(figsize=(16, 10))\n","    ax.bar(labels, perf, color=colors)\n","    plt.grid(True)\n","    plt.ylabel(metric)\n","    plt.savefig(f'{metric}_validation.png')\n","    #plt.show(block=False)\n","\n","\n","def print_performance_supervised(clf_name, acc, f1, y_test, output):\n","\n","    print(20 * '*')\n","    print(f'Performance for classifier: {clf_name}')\n","    print(f'Accuracy --> {acc}')\n","    print(f'F1 Score --> {f1}')\n","\n","    classes = np.unique(output)\n","    print(\"Classification report: \\n\", (classification_report(y_test, output)))\n","    cm = confusion_matrix(y_test, output, labels=classes, normalize='true')\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                  display_labels=classes)\n","    disp.plot()\n","    plt.savefig(f'{clf_name}_confusion.png')\n","    #plt.show(block=False)\n","    print(20 * '*')\n","\n","# unsupervised learning (clustering)\n","\n","\n","def sampling_silhouette(test_data, pred_labels, ns, runs=200):\n","    '''\n","\n","    :param test_data: input for clustering (N samples x M Features)\n","    :param pred_labels: output of clustering applied on test_data (N samples)\n","    :param ns: number of samples to take from test_data and pred_labels to make an estimate of silhouette score\n","    :param runs: number of ietartions over which to average\n","    :return: the average silhouette score over rhe number of runs\n","    '''\n","\n","    sscores = []\n","\n","    for r in range(runs):\n","\n","        idx = pd.Index(np.random.choice(test_data.reset_index().index, ns, replace=False))\n","\n","        x = test_data.iloc[idx]\n","        l = pred_labels[idx.values]\n","\n","        sscores.append(silhouette_score(x,l))\n","\n","    return np.mean(sscores)\n","\n","def kmeans_silhouette(data, min_cl, max_cl,initialization=\"k-means++\", estimated=False):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param max_cl: maximum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :param estimated: Boolean, whether to opt or not for an estimated version of silhouette score\n","    :return: number of clusters that maximizes the silhouette score\n","    '''\n","\n","    print(f'SILHOUETTE EVALUATION')\n","    silhouette_avg = []\n","    for num_clusters in list(range(min_cl, max_cl)):\n","\n","        print(f'Clustering for k={num_clusters}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:num_clusters,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeans = KMeans(init=init, n_clusters=num_clusters, n_init='auto')\n","        kmeans.fit_predict(data)\n","\n","        if not estimated:\n","            score = silhouette_score(data, kmeans.labels_)\n","        else:\n","            score = sampling_silhouette(data, kmeans.labels_, ns=1000)\n","        print(f'---- Score for k={num_clusters} --> {score}')\n","        silhouette_avg.append(score)\n","\n","        print(f'DONE')\n","\n","    best_k  = np.argmax(silhouette_avg)+min_cl\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.plot(np.arange(min_cl, max_cl), silhouette_avg, 'bx-')\n","    plt.xlabel('Values of K')\n","    plt.ylabel('Silhouette score')\n","    plt.title('Silhouette Analysis for Optimal k')\n","    _ = plt.xticks(np.arange(min_cl, max_cl))\n","    plt.grid(True)\n","    plt.savefig(f'Silhouette_Scores.png')\n","    plt.show(block=False)\n","\n","    return best_k\n","\n","def kmeans_helbow(data, max_cl, initialization=\"k-means++\", ):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :return: helbow plot\n","    '''\n","\n","    print(f'HELBOW RULE')\n","    distortions = []\n","    K = range(1, max_cl)\n","    for k in K:\n","        print(f'Clustering for k={k}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:k,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeanModel = KMeans(init=init, n_clusters=k, n_init='auto')\n","        kmeanModel.fit(data)\n","        distortions.append(kmeanModel.inertia_)\n","        print(f'DONE')\n","\n","    #plt.figure(figsize=(12, 8))\n","    fig, ax = plt.subplots(1, 1)\n","    ax.plot(K, distortions, 'bx-')\n","    plt.xlabel(r'k')\n","    plt.ylabel(r'Inertia')\n","    plt.title('Elbow Method for Optimal k')\n","    ax.grid(True)\n","    plt.savefig(f'Elbow_Rule.png')\n","    plt.show(block=False)\n","\n","    return ax\n","\n","def compute_unsupervised_performance(k, gt, test_data, pred_labels, ns):\n","\n","    sscore = sampling_silhouette(test_data, pred_labels, ns=ns)\n","    completeness = completeness_score(gt, pred_labels)\n","    homogeneity = homogeneity_score(gt, pred_labels)\n","\n","    print(20 * '#')\n","    print(f'Performance for clustering: k-means with k={k}')\n","    print(f'Estimated Silhouette Score --> {sscore}')\n","    print(f'Completeness --> {completeness}')\n","    print(f'Homogeneity --> {homogeneity}')\n","    print(20 * '#')"],"metadata":{"id":"5A7zcJZAGV-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Load and Preprocess/Visualize Train/Test Data"],"metadata":{"id":"og2qTObkJGZb"}},{"cell_type":"code","source":["# Column headers\n","columns_list = [\n","    \"Timestamp\", \"IMSI\", \"slice_id\", \"slice_prb\", \"scheduling_policy\", \"dl_mcs\",\n","    \"dl_n_samples\", \"dl_buffer [bytes]\", \"tx_brate downlink [Mbps]\", \"tx_pkts downlink\",\n","    \"dl_cqi\", \"ul_mcs\", \"ul_n_samples\", \"ul_buffer [bytes]\", \"rx_brate uplink [Mbps]\",\n","    \"rx_pkts uplink\", \"rx_errors uplink (%)\", \"ul_sinr\", \"sum_requested_prbs\", \"sum_granted_prbs\"\n","]\n","\n","# Dataset filenames\n","dataset_filenames = {\n","    \"training\": \"dataset_restart_training.pkl\",\n","    \"testing\": \"dataset_restart_testing.pkl\",\n","}\n","\n","rs = 42"],"metadata":{"id":"wEdk-AEkGoDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configurations\n","dataset_filename = dataset_filenames[\"training\"]\n","\n","# Load dataset\n","#train_dataset = load_dataset(train_filename)\n","dataset = load_dataset(dataset_filename)\n","\n","# Ensure the Timestamp column is in datetime format\n","#train_dataset['Timestamp'] = pd.to_datetime(train_dataset['Timestamp'], errors='coerce')\n","dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'], errors='coerce')\n"],"metadata":{"id":"mpK5ffcrGoGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test Dataset\n","\n","test_filename = 'dataset_restart_testing_fake.pkl'\n","test_dataset = pd.read_pickle(test_filename)\n","\n","# Ensure the Timestamp column is in datetime format\n","#train_dataset['Timestamp'] = pd.to_datetime(train_dataset['Timestamp'], errors='coerce')\n","test_dataset['Timestamp'] = pd.to_datetime(test_dataset['Timestamp'], errors='coerce')"],"metadata":{"id":"h4ymcRzoGmN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare Test Data for Performance Evaluation\n","\n","#Training\n","\n","X = dataset.drop([\"Timestamp\", \"IMSI\", \"slice_id\"], axis=1)\n","y =  dataset.loc[:, 'slice_id']\n","\n","#Test\n","X_test = test_dataset.drop([\"Timestamp\", \"IMSI\", \"slice_id\"], axis=1)\n","y_test =  test_dataset.loc[:, 'slice_id']\n","\n","X_norm, X_test_norm, stats_test = normalize_dataset(X, X_test)"],"metadata":{"id":"LJDgpow4IDE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Supervised Learning: Classification"],"metadata":{"id":"uVtOcrqyHLqh"}},{"cell_type":"code","source":["###### MODIFY THIS SECTION INTRODUCING YOUR CUSTOM CLASSIFIER(S)\n","\n","# Put here your final Classifier(s) with Hyper-Parameters tuned\n","classifiers = {\n","    \"Linear Regression\": RidgeClassifier(fit_intercept=True, solver='svd')\n","    }\n","\n","\n"],"metadata":{"id":"LN3sh0_hG2i1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (clf_name, clf) in enumerate(classifiers.items()):\n","\n","  clf.fit(X_norm, y) #train\n","\n","  output = clf.predict(X_test_norm) #predict\n","\n","  f1 = f1_score(y_test, output, average='micro')\n","  acc = accuracy_score(y_test, output)\n","  print_performance_supervised(clf_name, acc, f1, y_test, output)"],"metadata":{"id":"KJ6k_oVeG20v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Unsupervised Learning: K-Means Clustering"],"metadata":{"id":"JQQ_eaT3IfXO"}},{"cell_type":"code","source":["###### MODIFY THIS SECTION USING AD-HOC K FOR YOUR CUSTOM CLUSTERING CONFIGURATION\n","\n","k_test =[] #select best k as input hyper-parameter for clustering on test set"],"metadata":{"id":"bhC-eb4mImm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k in k_test:\n","\n","  kmeans_model = KMeans(n_clusters=k, init=\"k-means++\", n_init='auto')\n","\n","\n","  kmeans_model.fit(X_norm) # train the model\n","\n","  cluster_labels = kmeans_model.predict(X_test_norm) # assign label to test data\n","\n","  compute_unsupervised_performance(k, y_test, X_test_norm, cluster_labels, ns=1000)"],"metadata":{"id":"3ni7xENXIpTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"39xAQetTKQlW"},"execution_count":null,"outputs":[]}]}