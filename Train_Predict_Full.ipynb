{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-IwXIzjKLUEV","7TNyDwpMY4YP"],"authorship_tag":"ABX9TyNKm18GX/1HnJb+K7XFE19m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Use this notebook to Train & Tune supervised and unsupervised algorithms on Training Set, and to perform prediction.\n","\n","NB: save 1-d array of predictions in .npy files as it follows:\n","\n","\n","*  Supervised Learning with classifier \"clf\": **{clf_name}_test_prediction_supervised.npy**\n","*  Unsupervised Learning with k-means: **labels_test_prediction_k={k}_unsupervised.npy**\n","\n"],"metadata":{"id":"TfoDHMW9Keee"}},{"cell_type":"markdown","source":["#Import Libraries and Functions"],"metadata":{"id":"-IwXIzjKLUEV"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib as mpl\n","import numpy as np\n","from collections import defaultdict\n","\n","# Supervised Learning\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Clustering\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, silhouette_samples, completeness_score, homogeneity_score\n","\n","\n","# PREPROCESSING\n","\n","def load_dataset(filename):\n","    \"\"\" Load the dataset and handle errors \"\"\"\n","    try:\n","        dataset = pd.read_pickle(filename)\n","        print(f\"Dataset Loaded with {len(dataset)} rows!\")\n","        return dataset\n","    except Exception as e:\n","        print(f\"Could not load the dataset, exiting! Error: {e}\")\n","        exit(-1)\n","\n","\n","def split_and_save_dataset(dataset, extract_rate, filenames):\n","    \"\"\" Split dataset and save the samples \"\"\"\n","    # Sampling a fraction of the dataset\n","    sampled_df = dataset.sample(frac=extract_rate, random_state=42)\n","    remaining_df = dataset.drop(sampled_df.index)\n","\n","    # Resetting indices\n","    sampled_df = sampled_df.reset_index(drop=True)\n","    remaining_df = remaining_df.reset_index(drop=True)\n","\n","    # Save to pickle\n","    sampled_df.to_pickle(filenames['testing'])\n","    remaining_df.to_pickle(filenames['training'])\n","\n","    print(f\"Training dataset saved with {len(remaining_df)} rows!\")\n","    print(f\"Testing dataset saved with {len(sampled_df)} rows!\")\n","\n","\n","def plot_kpi(imsi_data, kpi_column, selected_imsi):\n","    \"\"\" Plot the KPI over time using a stem plot \"\"\"\n","    # Adjust 'Timestamp' to start from 0 using the smallest value as reference\n","    imsi_data['Adjusted_Timestamp'] = (imsi_data['Timestamp'] - imsi_data['Timestamp'].min()).dt.total_seconds()\n","\n","    # Plot\n","    plt.figure(figsize=(10, 6))\n","    plt.stem(imsi_data['Adjusted_Timestamp'], imsi_data[kpi_column], use_line_collection=True)\n","    plt.xlabel('Time [s]')  # Time in seconds\n","    plt.ylabel(kpi_column)\n","    plt.title(f'{kpi_column} Over Time for IMSI: {selected_imsi}')\n","    plt.grid(True)\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","\n","def plot_correlation_matrix(imsi_data):\n","    \"\"\" Plot the correlation matrix excluding certain columns \"\"\"\n","    # Drop unwanted columns\n","    data_to_correlate = imsi_data.drop(columns=[\"Timestamp\", \"IMSI\", \"slice_id\"], errors='ignore')\n","\n","    # Compute the correlation matrix\n","    corr_matrix = data_to_correlate.corr()\n","\n","    # Plot the correlation matrix\n","    plt.figure(figsize=(12, 8))\n","    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","    plt.title(\"Correlation Matrix for Selected IMSI\")\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","# TRAINING AND PREDICTION\n","\n","\n","def normalize_dataset(X_train, X_test):\n","    mean_x, std_x = X_train.mean(), X_train.std()\n","\n","    X_train_norm = (X_train - mean_x) / std_x\n","    X_test_norm = (X_test - mean_x) / std_x\n","\n","    stats_x = [mean_x, std_x]\n","\n","    return X_train_norm, X_test_norm, stats_x\n","\n","# supervised learning\n","\n","def grid_search(classifier, parameters, train, ground_truth, pred_input, cross_val=3):\n","\n","    gscv = GridSearchCV(classifier, parameters, cv=cross_val,\n","                        n_jobs=12, return_train_score=False,\n","                        verbose=5, scoring='accuracy')\n","    gscv.fit(train, ground_truth)\n","\n","    y_pred = gscv.predict(pred_input)\n","\n","    return gscv, y_pred\n","\n","\n","def get_bestpar_list(bestpar_df):\n","\n","\n","    dd = defaultdict(list)\n","    for index, row in bestpar_df.iterrows():  # list input dicts\n","\n","        bp = row['bestpars']\n","        for key, value in zip(list(bp.keys()), list(bp.values())):\n","            dd[key] = value\n","\n","    return dd\n","\n","def plot_metric_supervised(perf, metric, labels, colors=None):\n","\n","    fig, ax = plt.subplots(figsize=(16, 10))\n","    ax.bar(labels, perf, color=colors)\n","    plt.grid(True)\n","    plt.ylabel(metric)\n","    plt.savefig(f'{metric}_validation.png')\n","    #plt.show(block=False)\n","\n","\n","def print_performance_supervised(clf_name, acc, f1, y_test, output):\n","\n","    print(20 * '*')\n","    print(f'Performance for classifier: {clf_name}')\n","    print(f'Accuracy --> {acc}')\n","    print(f'F1 Score --> {f1}')\n","\n","    classes = np.unique(output)\n","    print(\"Classification report: \\n\", (classification_report(y_test, output)))\n","    cm = confusion_matrix(y_test, output, labels=classes, normalize='true')\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                  display_labels=classes)\n","    disp.plot()\n","    plt.savefig(f'{clf_name}_confusion.png')\n","    #plt.show(block=False)\n","    print(20 * '*')\n","\n","# unsupervised learning (clustering)\n","\n","\n","def sampling_silhouette(test_data, pred_labels, ns, runs=200):\n","    '''\n","\n","    :param test_data: input for clustering (N samples x M Features)\n","    :param pred_labels: output of clustering applied on test_data (N samples)\n","    :param ns: number of samples to take from test_data and pred_labels to make an estimate of silhouette score\n","    :param runs: number of ietartions over which to average\n","    :return: the average silhouette score over rhe number of runs\n","    '''\n","\n","    sscores = []\n","\n","    for r in range(runs):\n","\n","        idx = pd.Index(np.random.choice(test_data.reset_index().index, ns, replace=False))\n","\n","        x = test_data.iloc[idx]\n","        l = pred_labels[idx.values]\n","\n","        sscores.append(silhouette_score(x,l))\n","\n","    return np.mean(sscores)\n","\n","def kmeans_silhouette(data, min_cl, max_cl,initialization=\"k-means++\", estimated=False):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param max_cl: maximum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :param estimated: Boolean, whether to opt or not for an estimated version of silhouette score\n","    :return: number of clusters that maximizes the silhouette score\n","    '''\n","\n","    print(f'SILHOUETTE EVALUATION')\n","    silhouette_avg = []\n","    for num_clusters in list(range(min_cl, max_cl)):\n","\n","        print(f'Clustering for k={num_clusters}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:num_clusters,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeans = KMeans(init=init, n_clusters=num_clusters, n_init='auto')\n","        kmeans.fit_predict(data)\n","\n","        if not estimated:\n","            score = silhouette_score(data, kmeans.labels_)\n","        else:\n","            score = sampling_silhouette(data, kmeans.labels_, ns=1000)\n","        print(f'---- Score for k={num_clusters} --> {score}')\n","        silhouette_avg.append(score)\n","\n","        print(f'DONE')\n","\n","    best_k  = np.argmax(silhouette_avg)+min_cl\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.plot(np.arange(min_cl, max_cl), silhouette_avg, 'bx-')\n","    plt.xlabel('Values of K')\n","    plt.ylabel('Silhouette score')\n","    plt.title('Silhouette Analysis for Optimal k')\n","    _ = plt.xticks(np.arange(min_cl, max_cl))\n","    plt.grid(True)\n","    plt.savefig(f'Silhouette_Scores.png')\n","    plt.show(block=False)\n","\n","    return best_k\n","\n","def kmeans_helbow(data, max_cl, initialization=\"k-means++\", ):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :return: helbow plot\n","    '''\n","\n","    print(f'HELBOW RULE')\n","    distortions = []\n","    K = range(1, max_cl)\n","    for k in K:\n","        print(f'Clustering for k={k}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:k,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeanModel = KMeans(init=init, n_clusters=k, n_init='auto')\n","        kmeanModel.fit(data)\n","        distortions.append(kmeanModel.inertia_)\n","        print(f'DONE')\n","\n","    #plt.figure(figsize=(12, 8))\n","    fig, ax = plt.subplots(1, 1)\n","    ax.plot(K, distortions, 'bx-')\n","    plt.xlabel(r'k')\n","    plt.ylabel(r'Inertia')\n","    plt.title('Elbow Method for Optimal k')\n","    ax.grid(True)\n","    plt.savefig(f'Elbow_Rule.png')\n","    plt.show(block=False)\n","\n","    return ax\n","\n","def compute_unsupervised_performance(k, gt, test_data, pred_labels, ns):\n","\n","    sscore = sampling_silhouette(test_data, pred_labels, ns=ns)\n","    completeness = completeness_score(gt, pred_labels)\n","    homogeneity = homogeneity_score(gt, pred_labels)\n","\n","    print(20 * '#')\n","    print(f'Performance for clustering: k-means with k={k}')\n","    print(f'Estimated Silhouette Score --> {sscore}')\n","    print(f'Completeness --> {completeness}')\n","    print(f'Homogeneity --> {homogeneity}')\n","    print(20 * '#')"],"metadata":{"id":"iqBzbMqMKdI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bo_OqSPBKXTW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Load and Preprocess/Visualize Train Data"],"metadata":{"id":"_6nnVfPwLX-S"}},{"cell_type":"code","source":["# Column headers\n","columns_list = [\n","    \"Timestamp\", \"IMSI\", \"slice_id\", \"slice_prb\", \"scheduling_policy\", \"dl_mcs\",\n","    \"dl_n_samples\", \"dl_buffer [bytes]\", \"tx_brate downlink [Mbps]\", \"tx_pkts downlink\",\n","    \"dl_cqi\", \"ul_mcs\", \"ul_n_samples\", \"ul_buffer [bytes]\", \"rx_brate uplink [Mbps]\",\n","    \"rx_pkts uplink\", \"rx_errors uplink (%)\", \"ul_sinr\", \"sum_requested_prbs\", \"sum_granted_prbs\"\n","]\n","\n","# Dataset filenames\n","dataset_filenames = {\n","    \"training\": \"dataset_restart_training.pkl\",\n","    \"testing\": \"dataset_restart_testing.pkl\",\n","}\n","\n","rs = 42"],"metadata":{"id":"S1elyOhRLYdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Configurations\n","dataset_filename = dataset_filenames[\"testing\"] # use \"training\"\n","\n","# Load dataset\n","dataset = load_dataset(dataset_filename)\n","\n","# Ensure the Timestamp column is in datetime format\n","dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'], errors='coerce')"],"metadata":{"id":"61_VvuSVLaw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726069064223,"user_tz":-120,"elapsed":3056,"user":{"displayName":"Andrea PIMPINELLA","userId":"10265568976679300423"}},"outputId":"e5427aac-e615-4be1-8eae-e90857f51977"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Loaded with 1565185 rows!\n"]}]},{"cell_type":"code","source":["selected_imsi = 1010123456004  # Example IMSI, update as needed\n","selected_kpi = 'tx_pkts downlink'  # Example KPI, update as needed"],"metadata":{"id":"oKTDQcUCLvVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting\n","\n","# Filter the data for the selected IMSI\n","imsi_data = dataset[dataset['IMSI'] == selected_imsi]\n","\n","# Plot the KPI over time\n","plot_kpi(imsi_data, selected_kpi, selected_imsi)\n","\n","# Plot the correlation matrix\n","plot_correlation_matrix(imsi_data)"],"metadata":{"id":"cEcdymzqLz9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare Data for Training and Validation Evaluation\n","X, X_test, y, y_test = train_test_split(dataset.drop([\"Timestamp\", \"IMSI\", \"slice_id\"], axis=1),\n","                                                    dataset.loc[:, 'slice_id'],\n","                                                    test_size=0.2, random_state=rs)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n","                                                  random_state=rs)  # 0.25\n","\n","X_train_norm, X_val_norm, stats_val = normalize_dataset(X_train, X_val)\n","X_norm, X_test_norm, stats_test = normalize_dataset(X, X_test)"],"metadata":{"id":"5y-hkTbyL5Sd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Supervised Learning"],"metadata":{"id":"gQ53GSbqMF3m"}},{"cell_type":"code","source":["# Select Classsifiers\n","\n","classifiers = {\n","    \"Linear Regression\": RidgeClassifier(solver='svd'),\n","    \"Random Forest\": RandomForestClassifier(random_state=rs)\n","}"],"metadata":{"id":"XdInMZ25MMVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select Hyper-Parameters\n","params = {'Linear Regression': {'alpha': list(np.logspace(-2, 2, num=10))},\n","          'Random Forest': {'n_estimators': [50,200]}\n","          }\n","\n","# Validation\n","perf_cols = ['f1', 'accuracy', 'gscv','bestpars']\n","perf_avg = {\"Random Forest\": pd.DataFrame(columns=perf_cols),\n","            'Linear Regression': pd.DataFrame(columns=perf_cols)}"],"metadata":{"id":"-6cHNIb1MMXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utils\n","f1_list = []\n","acc_list = []\n","col = ['blue', 'red', 'green']\n","cross_val_k = 3 # number of inner cross validation iterations"],"metadata":{"id":"TfsBsQ_dMWyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (clf_name, clf) in enumerate(classifiers.items()):\n","\n","    print(10 * '-')\n","\n","    # Validation\n","\n","    print(f'Start GS procedure for {clf_name}...')\n","\n","    gscv, y_pred = grid_search(clf, params[clf_name],\n","                                X_train_norm, y_train, X_val_norm,\n","                                cross_val=cross_val_k)\n","\n","    print('... completed.')\n","\n","\n","    f1 = f1_score (y_val, y_pred, average='micro')\n","    acc = accuracy_score(y_val,y_pred)\n","    bestpar = gscv.best_params_\n","\n","    print(f'Best HP Set for {clf_name}: {bestpar}')\n","\n","    # Update Validation Performance\n","    vp = np.asarray([f1, acc, gscv, bestpar]).reshape(1, -1)\n","    perf_avg[clf_name] = pd.concat([perf_avg[clf_name],\n","                                    pd.DataFrame(columns=perf_cols, data=vp)])\n","\n","    # Save performance of cross-validation procedure\n","\n","    perf_avg[clf_name].to_pickle(f'{clf_name}_validation_supervised.pkl')\n","\n","    f1_list.append(perf_avg[clf_name]['f1'].values[0])\n","    acc_list.append(perf_avg[clf_name]['accuracy'].values[0])\n","\n","\n","    # Testing\n","\n","    print(f'First train the classifier with the optimized set of HP values...')\n","    clf_tuned = classifiers[clf_name].set_params(**bestpar)\n","    clf_tuned.fit(X_norm, y)\n","\n","    print(f'... and secondly perform prediction on test set...')\n","    output = clf_tuned.predict(X_test_norm)\n","\n","    with open(f'{clf_name}_test_prediction_supervised.npy', 'wb') as f:\n","        np.save(f, output)\n","\n","    print('... Done!')"],"metadata":{"id":"mgskjvJuMW1Y","colab":{"base_uri":"https://localhost:8080/","height":602},"executionInfo":{"status":"error","timestamp":1726069644696,"user_tz":-120,"elapsed":536773,"user":{"displayName":"Andrea PIMPINELLA","userId":"10265568976679300423"}},"outputId":"fea89239-3468-4791-833a-4694c2e9f64f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------\n","Start GS procedure for Linear Regression...\n","Fitting 3 folds for each of 10 candidates, totalling 30 fits\n","... completed.\n","Best HP Set for Linear Regression: {'alpha': 12.915496650148826}\n","First train the classifier with the optimized set of HP values...\n","... and secondly perform prediction on test set...\n","... Done!\n","----------\n","Start GS procedure for Random Forest...\n","Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-30b7d6d1be31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Start GS procedure for {clf_name}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     gscv, y_pred = grid_search(clf, params[clf_name],\n\u001b[0m\u001b[1;32m     10\u001b[0m                                 \u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                 cross_val=cross_val_k)\n","\u001b[0;32m<ipython-input-1-b64cef96c74b>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(classifier, parameters, train, ground_truth, pred_input, cross_val)\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         verbose=5, scoring='accuracy')\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgscv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    896\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    843\u001b[0m                     )\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    846\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#Unsupervised Learning"],"metadata":{"id":"JrEiUBxKMF7L"}},{"cell_type":"code","source":["k = None # Set to None for tuning of k, otherwise to int greater than 1 to perform clustering."],"metadata":{"id":"IrXB94K9MKUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use Helbow Rule to select best k\n","min_cl_km = 2\n","max_cl_km = 8\n","kmeans_helbow(X_norm, max_cl_km, initialization=\"k-means++\")"],"metadata":{"id":"7P2q7C1ZMxtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check how Silhouette Score varies with k\n","k_silhouette = kmeans_silhouette(X_norm, min_cl_km, max_cl_km,\n","                                 initialization = 'k-means++', estimated=True)\n","\n","print(f\"Best K Silhouette: {k_silhouette}\") # extract k with best sil coeff"],"metadata":{"id":"Y90d5W7NMxwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k_opt = None"],"metadata":{"id":"T_hQmV11DLJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if k_opt is not None:\n","\n","  # Once that k is tuned, use it to perform clustering and generate labels on Test Set\n","  kmeans_model = KMeans(n_clusters=k_opt, init=\"k-means++\", n_init='auto')\n","  kmeans_model.fit(X_norm)\n","  cluster_labels = kmeans_model.predict(X_test_norm)\n","\n","  with open(f'labels_test_prediction_k={k_opt}_unsupervised.npy', 'wb') as f:\n","      np.save(f, cluster_labels)"],"metadata":{"id":"foZvP_s_NRIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Unsupervised Learning: Explore Principal Component Analysis"],"metadata":{"id":"7TNyDwpMY4YP"}},{"cell_type":"code","source":["### PERFORM PCA to check how clusters look like in PC plane\n","\n","# Defining the number of principal components to generate\n","n = min(X_norm.shape[0], X_norm.shape[1])  # get maximum n of components accepted by scikit.PCA\n","\n","\n","# Finding principal components for the data\n","pca = PCA(n_components=n, random_state=42)\n","X_norm_pca = pd.DataFrame(pca.fit_transform(X_norm))\n"],"metadata":{"id":"qiNCuZAIMxq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The percentage of variance explained by each principal component\n","exp_var = pca.explained_variance_ratio_\n","\n","# visualize the Explained Individual Components\n","plt.figure(figsize=(10, 10))\n","plt.plot(range(1, X_norm.shape[1] + 1), exp_var.cumsum(),\n","          marker='o', linestyle='--')\n","plt.title(\"Explained Variances by Components\")\n","plt.xlabel(\"Number of Components\")\n","plt.ylabel(\"Cumulative Explained Variance\")\n","plt.grid(True)\n","plt.show(block=False)"],"metadata":{"id":"F1E28TbZQ-GE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the least number of components that can explain more than x% variance\n","xvar = 90\n","\n","sum = 0\n","for ix, i in enumerate(exp_var):\n","    sum = sum + i\n","    if (100 * sum > xvar):\n","        print(f\"Number of PCs that explain at least {xvar}% variance: \", ix + 1)\n","        nx = ix + 1\n","        break"],"metadata":{"id":"7aM-Ex78REaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_norm_pca_nx = X_norm_pca.iloc[:,:nx]\n","\n","fig, ax = plt.subplots(1,1,figsize=(10, 6))\n","\n","ax.scatter(X_norm_pca_nx.iloc[:, 0], X_norm_pca_nx.iloc[:, 1],\n","            marker='o', s=4)\n","\n","ticks = [-10,-5, 0, 5, 10]\n","ax.set_xticks(ticks, labels=[str(x) for x in ticks])\n","ax.set_yticks(ticks, labels=[str(x) for x in ticks])\n","ax.set_xlabel(r'PC1')\n","ax.set_ylabel(r'PC2')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"sSQK9xr0RFJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["centroids = pca.transform(kmeans_model.cluster_centers_)\n","train_labels = kmeans_model.labels_\n","colors_clusters = ['red','blue','green','black','purple','orange']\n","\n","fig, ax = plt.subplots(1, 1, figsize=(10,6))\n","\n","for i, tl in enumerate(np.unique(train_labels)):\n","\n","    idxs = train_labels == tl\n","    cl_data = X_norm_pca[idxs]\n","\n","    ax.scatter(cl_data.iloc[:, 0], cl_data.iloc[:, 1],\n","                color=colors_clusters[i], marker='o', s=4)\n","\n","    ax.scatter(\n","        centroids[tl, 0],\n","        centroids[tl, 1],\n","        marker=\"o\",\n","        s=169,\n","        linewidths=4,\n","        facecolor=colors_clusters[i],\n","        zorder=10, edgecolor='white', linewidth=3)\n","ax.set_xlabel(r'PC1')\n","ax.set_ylabel(r'PC2')\n","plt.grid(True)\n","plt.savefig(f'k={k}_PC1_vs_PC2.png', bbox_inches='tight')\n","plt.show()\n","\n","print('')"],"metadata":{"id":"MIyTKbpBP_FH"},"execution_count":null,"outputs":[]}]}