{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-IwXIzjKLUEV","_6nnVfPwLX-S","gQ53GSbqMF3m","JrEiUBxKMF7L","7TNyDwpMY4YP"],"authorship_tag":"ABX9TyOAj20JLe2aqaC/fR/JokHU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Use this notebook to Train & Tune supervised and unsupervised algorithms on Training Set, and to perform prediction.\n","\n","NB: the functions save_predictions_supervised and save_predictions_unsupervised will save 1-d array of predictions in .npy files as it follows:\n","\n","\n","*  Supervised Learning with classifier \"clf\": **{clf_name}_test_prediction_supervised.npy**\n","*  Unsupervised Learning with k-means: **labels_test_prediction_k={k}_unsupervised.npy**\n","\n"],"metadata":{"id":"TfoDHMW9Keee"}},{"cell_type":"markdown","source":["#Import Libraries and Functions"],"metadata":{"id":"-IwXIzjKLUEV"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib as mpl\n","import numpy as np\n","from collections import defaultdict\n","\n","# Supervised Learning\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.linear_model import RidgeClassifier\n","\n","# Clustering\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, silhouette_samples, completeness_score, homogeneity_score\n","\n","\n","# PREPROCESSING\n","\n","def load_dataset(filename):\n","    \"\"\" Load the dataset and handle errors \"\"\"\n","    try:\n","        dataset = pd.read_pickle(filename)\n","        print(f\"Dataset Loaded with {len(dataset)} rows!\")\n","        return dataset\n","    except Exception as e:\n","        print(f\"Could not load the dataset, exiting! Error: {e}\")\n","        exit(-1)\n","\n","\n","def split_and_save_dataset(dataset, extract_rate, filenames):\n","    \"\"\" Split dataset and save the samples \"\"\"\n","    # Sampling a fraction of the dataset\n","    sampled_df = dataset.sample(frac=extract_rate, random_state=42)\n","    remaining_df = dataset.drop(sampled_df.index)\n","\n","    # Resetting indices\n","    sampled_df = sampled_df.reset_index(drop=True)\n","    remaining_df = remaining_df.reset_index(drop=True)\n","\n","    # Save to pickle\n","    sampled_df.to_pickle(filenames['testing'])\n","    remaining_df.to_pickle(filenames['training'])\n","\n","    print(f\"Training dataset saved with {len(remaining_df)} rows!\")\n","    print(f\"Testing dataset saved with {len(sampled_df)} rows!\")\n","\n","\n","def plot_kpi(imsi_data, kpi_column, selected_imsi):\n","    \"\"\" Plot the KPI over time using a stem plot \"\"\"\n","    # Adjust 'Timestamp' to start from 0 using the smallest value as reference\n","    imsi_data['Adjusted_Timestamp'] = (imsi_data['Timestamp'] - imsi_data['Timestamp'].min()).dt.total_seconds()\n","\n","    # Plot\n","    plt.figure(figsize=(10, 6))\n","    plt.stem(imsi_data['Adjusted_Timestamp'], imsi_data[kpi_column], use_line_collection=True)\n","    plt.xlabel('Time [s]')  # Time in seconds\n","    plt.ylabel(kpi_column)\n","    plt.title(f'{kpi_column} Over Time for IMSI: {selected_imsi}')\n","    plt.grid(True)\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","\n","def plot_correlation_matrix(imsi_data):\n","    \"\"\" Plot the correlation matrix excluding certain columns \"\"\"\n","    # Drop unwanted columns\n","    data_to_correlate = imsi_data.drop(columns=[\"Timestamp\", \"IMSI\", \"slice_id\"], errors='ignore')\n","\n","    # Compute the correlation matrix\n","    corr_matrix = data_to_correlate.corr()\n","\n","    # Plot the correlation matrix\n","    plt.figure(figsize=(12, 8))\n","    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","    plt.title(\"Correlation Matrix for Selected IMSI\")\n","    plt.tight_layout()\n","    #plt.show(block=False)\n","\n","# TRAINING AND PREDICTION\n","\n","\n","def normalize_dataset(X_train, X_test):\n","    mean_x, std_x = X_train.mean(), X_train.std()\n","\n","    X_train_norm = (X_train - mean_x) / std_x\n","    X_test_norm = (X_test - mean_x) / std_x\n","\n","    stats_x = [mean_x, std_x]\n","\n","    return X_train_norm, X_test_norm, stats_x\n","\n","# supervised learning\n","\n","def grid_search(classifier, parameters, train, ground_truth, pred_input, cross_val=3):\n","\n","    gscv = GridSearchCV(classifier, parameters, cv=cross_val,\n","                        n_jobs=12, return_train_score=False,\n","                        verbose=5, scoring='accuracy')\n","    gscv.fit(train, ground_truth)\n","\n","    y_pred = gscv.predict(pred_input)\n","\n","    return gscv, y_pred\n","\n","\n","def get_bestpar_list(bestpar_df):\n","\n","\n","    dd = defaultdict(list)\n","    for index, row in bestpar_df.iterrows():  # list input dicts\n","\n","        bp = row['bestpars']\n","        for key, value in zip(list(bp.keys()), list(bp.values())):\n","            dd[key] = value\n","\n","    return dd\n","\n","def plot_metric_supervised(perf, metric, labels, colors=None):\n","\n","    fig, ax = plt.subplots(figsize=(16, 10))\n","    ax.bar(labels, perf, color=colors)\n","    plt.grid(True)\n","    plt.ylabel(metric)\n","    plt.savefig(f'{metric}_validation.png')\n","    #plt.show(block=False)\n","\n","\n","def print_performance_supervised(clf_name, acc, f1, y_test, output):\n","\n","    print(20 * '*')\n","    print(f'Performance for classifier: {clf_name}')\n","    print(f'Accuracy --> {acc}')\n","    print(f'F1 Score --> {f1}')\n","\n","    classes = np.unique(output)\n","    print(\"Classification report: \\n\", (classification_report(y_test, output)))\n","    cm = confusion_matrix(y_test, output, labels=classes, normalize='true')\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                  display_labels=classes)\n","    disp.plot()\n","    plt.savefig(f'{clf_name}_confusion.png')\n","    #plt.show(block=False)\n","    print(20 * '*')\n","\n","def save_predictions_supervised(classifier_name, predictions):\n","\n","  with open(f'{classifier_name}_test_prediction_supervised.npy', 'wb') as f:\n","      np.save(f, predictions)\n","\n","  print('... Done!')\n","\n","\n","# unsupervised learning (clustering)\n","\n","\n","def sampling_silhouette(test_data, pred_labels, ns, runs=200):\n","    '''\n","\n","    :param test_data: input for clustering (N samples x M Features)\n","    :param pred_labels: output of clustering applied on test_data (N samples)\n","    :param ns: number of samples to take from test_data and pred_labels to make an estimate of silhouette score\n","    :param runs: number of ietartions over which to average\n","    :return: the average silhouette score over rhe number of runs\n","    '''\n","\n","    sscores = []\n","\n","    for r in range(runs):\n","\n","        idx = pd.Index(np.random.choice(test_data.reset_index().index, ns, replace=False))\n","\n","        x = test_data.iloc[idx]\n","        l = pred_labels[idx.values]\n","\n","        sscores.append(silhouette_score(x,l))\n","\n","    return np.mean(sscores)\n","\n","def kmeans_silhouette(data, min_cl, max_cl,initialization=\"k-means++\", estimated=False):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param max_cl: maximum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :param estimated: Boolean, whether to opt or not for an estimated version of silhouette score\n","    :return: number of clusters that maximizes the silhouette score\n","    '''\n","\n","    print(f'SILHOUETTE EVALUATION')\n","    silhouette_avg = []\n","    for num_clusters in list(range(min_cl, max_cl)):\n","\n","        print(f'Clustering for k={num_clusters}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:num_clusters,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeans = KMeans(init=init, n_clusters=num_clusters, n_init='auto')\n","        kmeans.fit_predict(data)\n","\n","        if not estimated:\n","            score = silhouette_score(data, kmeans.labels_)\n","        else:\n","            score = sampling_silhouette(data, kmeans.labels_, ns=1000)\n","        print(f'---- Score for k={num_clusters} --> {score}')\n","        silhouette_avg.append(score)\n","\n","        print(f'DONE')\n","\n","    best_k  = np.argmax(silhouette_avg)+min_cl\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.plot(np.arange(min_cl, max_cl), silhouette_avg, 'bx-')\n","    plt.xlabel('Values of K')\n","    plt.ylabel('Silhouette score')\n","    plt.title('Silhouette Analysis for Optimal k')\n","    _ = plt.xticks(np.arange(min_cl, max_cl))\n","    plt.grid(True)\n","    plt.savefig(f'Silhouette_Scores.png')\n","    plt.show(block=False)\n","\n","    return best_k\n","\n","def kmeans_helbow(data, max_cl, initialization=\"k-means++\", ):\n","\n","    '''\n","\n","    :param data: input for clustering (N samples x M Features)\n","    :param min_cl: minimum number of clusters to test\n","    :param initialization: centroids initialization for k means algorithm. Check sciki-learn ref for more info.\n","    :return: helbow plot\n","    '''\n","\n","    print(f'HELBOW RULE')\n","    distortions = []\n","    K = range(1, max_cl)\n","    for k in K:\n","        print(f'Clustering for k={k}...')\n","\n","        if initialization not in ['k-means++', 'random']:\n","            init = initialization[0][:k,:initialization[1]] #select first k components as initializing centroids\n","        else:\n","            init = initialization\n","\n","        kmeanModel = KMeans(init=init, n_clusters=k, n_init='auto')\n","        kmeanModel.fit(data)\n","        distortions.append(kmeanModel.inertia_)\n","        print(f'DONE')\n","\n","    #plt.figure(figsize=(12, 8))\n","    fig, ax = plt.subplots(1, 1)\n","    ax.plot(K, distortions, 'bx-')\n","    plt.xlabel(r'k')\n","    plt.ylabel(r'Inertia')\n","    plt.title('Elbow Method for Optimal k')\n","    ax.grid(True)\n","    plt.savefig(f'Elbow_Rule.png')\n","    plt.show(block=False)\n","\n","    return ax\n","\n","def compute_unsupervised_performance(k, gt, test_data, pred_labels, ns):\n","\n","    sscore = sampling_silhouette(test_data, pred_labels, ns=ns)\n","    completeness = completeness_score(gt, pred_labels)\n","    homogeneity = homogeneity_score(gt, pred_labels)\n","\n","    print(20 * '#')\n","    print(f'Performance for clustering: k-means with k={k}')\n","    print(f'Estimated Silhouette Score --> {sscore}')\n","    print(f'Completeness --> {completeness}')\n","    print(f'Homogeneity --> {homogeneity}')\n","    print(20 * '#')\n","\n","def save_predictions_supervised(k, cluster_labels):\n","\n","  with open(f'labels_test_prediction_k={k}_unsupervised.npy', 'wb') as f:\n","      np.save(f, cluster_labels)\n","\n","  print('... Done!')\n","\n","\n"],"metadata":{"id":"iqBzbMqMKdI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bo_OqSPBKXTW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Load and Preprocess/Visualize Train Data"],"metadata":{"id":"_6nnVfPwLX-S"}},{"cell_type":"code","source":["# Column headers\n","columns_list = [\n","    \"Timestamp\", \"IMSI\", \"slice_id\", \"slice_prb\", \"scheduling_policy\", \"dl_mcs\",\n","    \"dl_n_samples\", \"dl_buffer [bytes]\", \"tx_brate downlink [Mbps]\", \"tx_pkts downlink\",\n","    \"dl_cqi\", \"ul_mcs\", \"ul_n_samples\", \"ul_buffer [bytes]\", \"rx_brate uplink [Mbps]\",\n","    \"rx_pkts uplink\", \"rx_errors uplink (%)\", \"ul_sinr\", \"sum_requested_prbs\", \"sum_granted_prbs\"\n","]\n","\n","# Dataset filenames\n","dataset_filenames = {\n","    \"training\": \"dataset_restart_training.pkl\",\n","    \"testing\": \"dataset_restart_testing.pkl\",\n","}\n","\n","rs = 42"],"metadata":{"id":"S1elyOhRLYdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Configurations\n","dataset_filename = dataset_filenames[\"training\"] # use \"training\"\n","\n","# Load dataset\n","dataset = load_dataset(dataset_filename)\n","\n","# Ensure the Timestamp column is in datetime format\n","dataset['Timestamp'] = pd.to_datetime(dataset['Timestamp'], errors='coerce')"],"metadata":{"id":"61_VvuSVLaw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_imsi = 1010123456004  # Example IMSI, update as needed\n","selected_kpi = 'tx_pkts downlink'  # Example KPI, update as needed"],"metadata":{"id":"oKTDQcUCLvVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting\n","\n","# Filter the data for the selected IMSI\n","imsi_data = dataset[dataset['IMSI'] == selected_imsi]\n","\n","# Plot the KPI over time\n","plot_kpi(imsi_data, selected_kpi, selected_imsi)\n","\n","# Plot the correlation matrix\n","plot_correlation_matrix(imsi_data)"],"metadata":{"id":"cEcdymzqLz9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare Data for Training and Validation Evaluation\n","X, X_test, y, y_test = train_test_split(dataset.drop([\"Timestamp\", \"IMSI\", \"slice_id\"], axis=1),\n","                                                    dataset.loc[:, 'slice_id'],\n","                                                    test_size=0.2, random_state=rs)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n","                                                  random_state=rs)  # 0.25\n","\n","X_train_norm, X_val_norm, stats_val = normalize_dataset(X_train, X_val)\n","X_norm, X_test_norm, stats_test = normalize_dataset(X, X_test)"],"metadata":{"id":"5y-hkTbyL5Sd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Supervised Learning"],"metadata":{"id":"gQ53GSbqMF3m"}},{"cell_type":"code","source":["# Select Classsifiers\n","\n","classifiers = {\n","    \"Linear Regression\": RidgeClassifier(solver='svd'),\n","}\n","\n","# Other classifiers available at: https://scikit-learn.org/stable/supervised_learning.html"],"metadata":{"id":"XdInMZ25MMVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select Hyper-Parameters\n","params = {'Linear Regression': []} #choose parameters to optimize\n"],"metadata":{"id":"-6cHNIb1MMXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example to predict with plain logistic regression\n","\n","for i, (clf_name, clf) in enumerate(classifiers.items()):\n","\n","  print(f'Train classifier on training set...')\n","  clf.fit(X_norm, y)\n","\n","  print(f'Perform prediction on test set...')\n","  output = clf.predict(X_test_norm)\n","\n","  save_predictions_supervised(clf_name, output)"],"metadata":{"id":"M4E4rHi2_hLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (clf_name, clf) in enumerate(classifiers.items()):\n","\n","    print(10 * '-')\n","\n","    # Validation\n","\n","    ######\n","    # Put Validation Logic Here\n","\n","    ######\n","\n","    # Testing\n","\n","    ######\n","    # Put Testing Logic Here\n","\n","    ######\n","\n","    # save predictions: save_predictions_supervised(clf_name, output)"],"metadata":{"id":"mgskjvJuMW1Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Unsupervised Learning (via k-means)"],"metadata":{"id":"JrEiUBxKMF7L"}},{"cell_type":"code","source":["k = None # Set to None for tuning of k, otherwise to int greater than 1 to perform clustering."],"metadata":{"id":"IrXB94K9MKUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use function kmeans_helbow to select best k\n","min_cl_km = 2\n","max_cl_km = 8\n"],"metadata":{"id":"7P2q7C1ZMxtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check how Silhouette Score varies with k with function kmeans_silhouette\n","\n","# k_silhouette =\n","\n","# print(f\"Best K Silhouette: {k_silhouette}\") # extract k with best sil coeff"],"metadata":{"id":"Y90d5W7NMxwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if k is not None:\n","\n","  # Once that k is tuned, use it to perform clustering and generate labels on Test Set\n","\n","  ######\n","  # Put Clustering Logic Here\n","\n","  ######\n","\n","  # save_predictions_supervised(k, output)\n"],"metadata":{"id":"foZvP_s_NRIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Unsupervised Learning: Explore Principal Component Analysis (Optional)"],"metadata":{"id":"7TNyDwpMY4YP"}},{"cell_type":"code","source":["### PERFORM PCA to check how clusters look like in PC plane\n","\n","# Defining the number of principal components to generate\n","n = min(X_norm.shape[0], X_norm.shape[1])  # get maximum n of components accepted by scikit.PCA\n","\n","\n","# Finding principal components for the data\n","pca = PCA(n_components=n, random_state=42)\n","X_norm_pca = pd.DataFrame(pca.fit_transform(X_norm))\n"],"metadata":{"id":"qiNCuZAIMxq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get percentages of variance explained by each principal component\n","# exp_var =\n","\n","# Visualize the Cumulative Sum of Explained Variance\n","plt.figure(figsize=(10, 10))\n","\n","plt.show(block=False)"],"metadata":{"id":"F1E28TbZQ-GE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the least number of components that can explain more than x% variance\n","xvar = 90\n"],"metadata":{"id":"7aM-Ex78REaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make a scatter PLot of 1st vs 2nd components\n"],"metadata":{"id":"sSQK9xr0RFJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make a scatter PLot of 1st vs 2nd components, where data points are labelled according to the associated cluster\n","# NB: also the centroids of the produced clustering configuration can be projected on the Principal plane\n","# applying the function transform() to the trained pca algorithm, giving as input the centroids\n","\n"],"metadata":{"id":"MIyTKbpBP_FH"},"execution_count":null,"outputs":[]}]}